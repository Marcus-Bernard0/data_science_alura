{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marcus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master('local[*]')\\\n",
    "    .appName('WordCloud')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tratamento das palavras\n",
    "## substituindo http por vazio, @ por vazio, rt por vazio\n",
    "def trata_tweets(df):\n",
    "    words = df \\\n",
    "        .select(f.explode(f.split(f.lower('_c0'), \" \")) \\\n",
    "        .alias(\"word\")) \\\n",
    "        .withColumn('word', f.regexp_replace('word', r'http\\S+', '')) \\\n",
    "        .withColumn('word', f.regexp_replace('word', r'@\\w+', '')) \\\n",
    "        .withColumn('word', f.regexp_replace('word', 'rt', '')) \\\n",
    "        .na.replace('', None) \\\n",
    "        .na.drop()\n",
    "    return words\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " O motivo é que talvez seja interessante desconsiderarmos a palavra \"futebol\" na nossa análise. Como estamos filtrando os tweets pela \"futebol\", ela aparecerá de qualquer maneira no nosso gráfico, então, queremos saber os assuntos que estão orbitando essa palavra nos tweets.\n",
    "\n",
    "For : Para as palavras não ficarem juntas umas as outras\\\n",
    "\n",
    "Limpa o eixo \\\n",
    " ```plt.cla()```\n",
    "\n",
    "Oculta as marcações dos eixos\\\n",
    "``` plt.axis('off') ```\n",
    "\n",
    "Utilizado para exibir os dados como uma imagem\\\n",
    "``` plt.imshow(wordcloud)```\n",
    "\n",
    "Mostrando a nossa word cloud no output do notebook\\\n",
    "```display.display(plt.gcf())```\n",
    "\n",
    "Limpa output do notebook\\\n",
    "```display.clear_output(wait=True) ``` \n",
    "\n",
    "Limpa de 5 em 5 segundos e gera o gráfico de novo, considerando os tweets do momento\\\n",
    "```time.sleep(5)```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Unable to infer schema for CSV at . It must be specified manually",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/marcus/Área de Trabalho/Clone GitHub/data_science_alura/24 - streaming de dados/word_cloud.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marcus/%C3%81rea%20de%20Trabalho/Clone%20GitHub/data_science_alura/24%20-%20streaming%20de%20dados/word_cloud.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marcus/%C3%81rea%20de%20Trabalho/Clone%20GitHub/data_science_alura/24%20-%20streaming%20de%20dados/word_cloud.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:      \u001b[39m# Este try/except foi colocado para tratar os erros que aparecem quando interrompemos o processo\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/marcus/%C3%81rea%20de%20Trabalho/Clone%20GitHub/data_science_alura/24%20-%20streaming%20de%20dados/word_cloud.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         words \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mcsv(\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mcsv\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)          \u001b[39m# Lendo o conjunto de arquivos CSV na pasta /csv\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marcus/%C3%81rea%20de%20Trabalho/Clone%20GitHub/data_science_alura/24%20-%20streaming%20de%20dados/word_cloud.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         words \u001b[39m=\u001b[39m trata_tweets(words)                                 \u001b[39m# Aplicando nossa função de tratamento\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marcus/%C3%81rea%20de%20Trabalho/Clone%20GitHub/data_science_alura/24%20-%20streaming%20de%20dados/word_cloud.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         rows \u001b[39m=\u001b[39m words\u001b[39m.\u001b[39mcollect()                                      \u001b[39m# Transformando o DataFrame em uma lista de linhas [1]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py:535\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m    534\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mcsv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mtoSeq(path)))\n\u001b[1;32m    536\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    538\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Unable to infer schema for CSV at . It must be specified manually"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stops = stopwords.words('portuguese')                               # Criando uma variável com as Stop Word em português. Se for utilizar tweets em inglês basta modificar 'portuguese' para 'english'\n",
    "stops.append('futebol')                                             # Pode ser interessante retirar a palavra utilizada na pesquisa\n",
    "plt.figure(figsize=(20, 10))                                        # Cria a figura e defini o tamanho dela (largura, altura)\n",
    "\n",
    "while True:\n",
    "    try:      # Este try/except foi colocado para tratar os erros que aparecem quando interrompemos o processo\n",
    "        words = spark.read.csv('.\\csv', encoding='utf-8')          # Lendo o conjunto de arquivos CSV na pasta /csv\n",
    "        words = trata_tweets(words)                                 # Aplicando nossa função de tratamento\n",
    "        rows = words.collect()                                      # Transformando o DataFrame em uma lista de linhas [1]\n",
    "        all_words = ''\n",
    "        for row in rows:\n",
    "            all_words = all_words + ' ' + row['word']\n",
    "\n",
    "        wordcloud = WordCloud(stopwords=stops,\n",
    "                              background_color=\"black\",\n",
    "                              width=1920,\n",
    "                              height=1080,\n",
    "                              max_words=100\n",
    "                              ).generate(all_words)                 # Word cloud simples. Mais detalhes em [2]\n",
    "\n",
    "        plt.cla()                                                   # Limpa os eixos do gráfico\n",
    "        plt.axis('off')                                             # Oculta as marcações dos eixos\n",
    "        plt.imshow(wordcloud)                                       # Utilizado para exibir os dados como uma imagem\n",
    "        plt.savefig(\"twitter.png\", format=\"png\")\n",
    "        display.display(plt.gcf())                                  # Mostrando a nossa word cloud no output do notebook\n",
    "        display.clear_output(wait=True)                             # Limpa o output do notebook\n",
    "        time.sleep(5)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad4078168b69d66fe05634f3b7be757ef4015f974a20b20ffdc4b2de487ad266"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
